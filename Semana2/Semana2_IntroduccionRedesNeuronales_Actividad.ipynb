{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Introducción a las redes neuronales\n",
    "\n",
    "## Actividad 2\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "\n",
    "\n",
    "En esta actividad vamos a estudiar una primera aproximación a los modelos de redes neuronales, utilizando como base el modelo de regresión logística.\n",
    "\n",
    "\n",
    "#### Integrantes:\n",
    "    - Daniela Rodriguez Otálora\n",
    "    - Fernando Pérez Moreno\n",
    "    - Luis Jorge García Camargo\n",
    "    - Diego Ojeda Vargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos paquetes iniciales que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problema de clasificación: riesgo de default\n",
    "\n",
    "Examinemos los datos con lo cuales ya estamos familiarizados:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_1 = pd.read_csv(\"germancredit.csv\")\n",
    "credit_1 = pd.get_dummies(credit_1, columns=['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'], prefix = ['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'])\n",
    "X = credit_1.iloc[:, 1:62]\n",
    "Y = credit_1.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de CE, CP:  (600,) (400,)\n",
      "Observaciones de la clase positiva en entrenamiento: 180 y en prueba: 120\n"
     ]
    }
   ],
   "source": [
    "CE_x, CP_x, CE_y, CP_y = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "print(\"Tamaño de CE, CP: \", CE_y.shape, CP_y.shape)\n",
    "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(CE_y)) +\" y en prueba: \" +str(sum(CP_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de una neurona Sigmoide\n",
    "\n",
    "Una neurona Sigmoide puede ser vista como un perceptrón *suavizado* que recibe una señal y entonces se activa. Al activarse, transforma la señal para entender mejor el mensaje. Esta transformación la ejecuta a partir de la fucnión Sigmoide.\n",
    "\n",
    "Si tomamos la señal como un conjunto de datos de entrada y el mensaje como la predicción de un valor, la función de activación jugará el papel de transformadora de los datos de entrada en aquello que se quiere entender/predecir, que además replica un modelo logit con la función de activación sigmoide.\n",
    "\n",
    "A continuación construiremos un clasificador de regresión logística bajo la perspectiva de una red neuronal, estudiando la arquitectura general de un algoritmo de aprendizaje. De esta manera, necesitaremos incluir la inicialización de los parámetros, el cálculo de la función de coste y su gradiente, y utilizar un algoritmo de optimización como por ejemplo el descenso en la dirección del gradiente (GD)\n",
    "\n",
    "**Formulación del algoritmo**:\n",
    "\n",
    "Para un ejemplo $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoide(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "El coste se calcula sumando sobre todos los ejemplos de entrenamiento:\n",
    "$$ L = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construimos las partes del algoritmo  \n",
    "\n",
    "- Inicializar los parámetros del modelo\n",
    "- Bucle:\n",
    "    - Calcular la pérdida actual (propagación hacia delante)\n",
    "    - Calcular el gradiente actual (retro-propagación)\n",
    "    - Actualizar los parámetros (descenso en la dirección del gradiente)\n",
    "\n",
    "\n",
    "### Ejercicio 2.1\n",
    "Implemente la funcion `sigmoide()` $$\\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$ Para ello puede utilizar np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    z: Un escalar o arreglo numpy de cualquier tamaño\n",
    "    Output:\n",
    "    s: sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-(z)))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoide([99,1,0,-1,-99]) = [1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
      " 1.01122149e-43]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoide([99,1,0,-1,-99]) = \" + str(sigmoide(np.array([99,1,0,-1,-99]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> sigmoide([99,1,0,-1,-99])    = </td>\n",
    "<td> [ 1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
    " 1.01122149e-43] </td> \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2 \n",
    "\n",
    "Debemos inicializar los parámetros a cero. Puede utilizar la funcion np.zeros(), apoyandose en la documentación de la biblioteca Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_ceros(dim):\n",
    "    \"\"\"\n",
    "    Esta función crea un vector de ceros de dimensión (dim, 1) para w e inicializa b a 0.\n",
    "    Input:\n",
    "    dim: tamaño del vector w (número de parámetros para este caso)\n",
    "    Output:\n",
    "    w: vector inicializado de tamaño (dim, 1)\n",
    "    b: escalar inicializado (corresponde con el sesgo)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "b = 1\n"
     ]
    }
   ],
   "source": [
    "dim = 6\n",
    "w, b = inicializa_ceros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "<tr>\n",
    "<td>   w   </td>\n",
    "<td> [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   b   </td>\n",
    "<td> 0 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3 \n",
    "#### Propagación hacia delante y hacia atrás\n",
    "\n",
    "Una vez los estimadores están inicializados, se pueden implementar los pasos de propagación hacia \"delante\" y hacia \"atrás\" para el aprendizaje automático. \n",
    "\n",
    "La propagación hacia delante consiste en calcular la función de activación sigmoide sobre la combinacón lineal de los patrones y los coeficientes inciales. \n",
    "\n",
    "Luego la propagación hacia atrás, o *retro-propagación*, es el paso más importante, donde utilizamos el gradiente de la función del error o de pérdida para actualizar los coeficientes. \n",
    "\n",
    "Este procedimiento se repite iterativamente replicando el procediemiento de descenso en la dirección del gradiente o *Gradient Descent* (GD).\n",
    "\n",
    "A continuación implemente la función `propaga()` que calcula la función de coste y su gradiente.\n",
    "\n",
    "**Ayuda**:\n",
    "\n",
    "Propagación hacia delante:\n",
    "- Se tiene $X$\n",
    "- Se calcula $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Se calcula la función de coste/pérdida: $L = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Para la retro-propagación, tenemos que calcular la derivada parcial de *L* con respecto a nuestros coeficientes $(w,b)$:  \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$\n",
    "\n",
    "*Nota:* Para el cálculo de estas derivadas debemos hacer uso de la regla de la cadena. \n",
    "\n",
    "Esto es, dado $Z=w^T X + b$, se tiene que $$\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial Z} = \\bigg(\\frac{-Y}{A}+\\frac{1-Y}{1-A}\\bigg) (A \\cdot (1-A)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.ones((6,1))+np.ones((6,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propaga(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implemente la función de coste y su gradiente para la propagación\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    Output:\n",
    "    coste: coste negativo de log-verosimilitud para la regresión logística\n",
    "    dw: gradiente de la pérdida con respecto a w, con las mismas dimensiones que w\n",
    "    db: gradiente de la pérdida con respecto a b, con las mismas dimensiones que b\n",
    "    \n",
    "    (Sugerencia: utilice las funciones np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoide(np.dot(np.transpose(w),X)+b)                # compute la activación\n",
    "    coste =   -1/m*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))              # compute el coste\n",
    "\n",
    "    dw = 1/m*np.dot(X,np.transpose(A-Y))\n",
    "    db = 1/m*np.sum(A-Y)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(coste)\n",
    "    assert(coste.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[65.48251839]\n",
      " [29.66675568]]\n",
      "db = 0.348980796447886\n",
      "coste = 9.752716367426284\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[0.1],[0.1]]), 0.5, np.array([[66.,99.,-33.],[32.,55.,-2.1]]), np.array([[0,0,1]])\n",
    "grads, coste = propaga(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"coste = \" + str(coste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\">\n",
    "<tr>\n",
    "<td>   dw   </td>\n",
    "<td> [[65.48251839]\n",
    " [29.66675568]]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   db   </td>\n",
    "<td> 0.348980796447886 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   cost   </td>\n",
    "<td> 9.752716367426284 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4 \n",
    "#### Optimización\n",
    "\n",
    "- Se tienen los parámetros inicializados.\n",
    "- También se tiene el código para calcular la función de coste y su gradiente.\n",
    "- Ahora se quieren actualizar los parámetros utilizando el GD.\n",
    "\n",
    "Escriba la función de optimización para aprender $w$ y $b$ minimizando la función de coste $L$. \n",
    "\n",
    "Para un parámetro $\\theta$, la regla de actualización es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, donde $\\alpha$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimiza(w, b, X, Y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Esta función optimiza w y b implementando el algoritmo de GD\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    num_iter: número de iteracionespara el bucle de optimización\n",
    "    tasa: tasa de aprendizaje para la regla de actualización del GD\n",
    "    print_cost: True para imprimir la pérdida cada 100 iteraciones\n",
    "    Output:\n",
    "    params: diccionario con los pesos w y el sesgo b\n",
    "    grads: diccionario con los gradientes de los pesos y el sesgo con respecto a la función de pérdida\n",
    "    costes: lista de todos los costes calculados durante la optimización, usados para graficar la curva de aprendizaje.\n",
    "    \n",
    "    Sugerencia: puede escribir dos pasos e iterar sobre ellos:\n",
    "        1) Calcule el coste y el gradiente de los parámetros actuales. Use propaga().\n",
    "        2) Actualize los parámetros usando la regla del GD para w y b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costes = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        \n",
    "        # Computación del coste y el gradiente \n",
    "        grads, coste = propaga(w, b, X, Y)\n",
    "        \n",
    "        # Recupere las derivadas de grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Actualize la regla \n",
    "        w = w - tasa*dw\n",
    "        b = b - tasa*db\n",
    "        \n",
    "        # Guarde los costes\n",
    "        if i % 100 == 0:\n",
    "            costes.append(coste)\n",
    "        \n",
    "        # Se muestra el coste cada 100 iteraciones de entrenamiento\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Coste tras la iteración %i: %f\" %(i, coste))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-0.07262234]\n",
      " [ 0.02112647]]\n",
      "b = 0.49898148713402446\n",
      "dw = [[1.42076721]\n",
      " [0.43496446]]\n",
      "db = -0.007821662502973652\n"
     ]
    }
   ],
   "source": [
    "params, grads, costes = optimiza(w, b, X, Y, num_iter= 10, tasa = 0.001, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:  \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> w </td>\n",
    "<td>[[-0.07262234]\n",
    " [ 0.02112647]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> b </td>\n",
    "<td> 0.49898148713402446 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> dw </td>\n",
    "<td> [[1.42076721]\n",
    " [0.43496446]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> db </td>\n",
    "<td> -0.007821662502973652 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.5\n",
    "\n",
    "La función anterior aprende los parámetros w y b, que se pueden usar para predecir sobre el conjunto de datos X. \n",
    "\n",
    "Hay dos pasos para calcular las predicciones:\n",
    "\n",
    "1. Calcular $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Converir a 0 las entradas de $a$ (si la activación es <= 0.5) o 1 (si la activación es > 0.5), guarde las predicciones en un vector `Y_pred`.  \n",
    "\n",
    "Ahora implemente la función `pred()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, b, X):\n",
    "    '''\n",
    "    Prediga si una etiqueta es 0 o 1 usando los parámetros de regresión logística aprendidos (w, b)\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Output:\n",
    "    Y_pred: vector con todas las predicciones (0/1) para los ejemplos en X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_pred = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute el vector \"A\" prediciendo las probabilidades de que la imagen contenga un frailejon\n",
    "    A = sigmoide(np.dot(np.transpose(w),X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convierta las probabilidades A[0,i] a predicciones p[0,i]\n",
    "        Y_pred[0,i] = round(A[0,i],0)\n",
    "    \n",
    "    assert(Y_pred.shape == (1, m))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicciones = [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.12],[0.23]])\n",
    "b = -0.09\n",
    "X = np.array([[3.1,-2.9,0.2],[1.9,1.8,-0.09]])\n",
    "print (\"predicciones = \" + str(pred(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> predicciones   </td>\n",
    "<td>[[ 1.  0.  0.]]  </td>  \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.6\n",
    "#### Combine todas las funciones \n",
    "\n",
    "Ahora juntemos todos los bloques que ha programado arriba.\n",
    "\n",
    "Implemente la función del modelo \"madre\". Use la siguiente notación:\n",
    "    - YP_pred para las predicciones sobre el conjunto de prueba\n",
    "    - YE_pred para las predicciones sobre el conjunto de entrenamiento\n",
    "    - w, costes, grads para las salidas de optimiza()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Construye el modelo de regresión logística llamando las funciones implementadas anteriormente\n",
    "    Output:\n",
    "    d: diccionario con la información sobre el modelo.\n",
    "    \"\"\"\n",
    "    dim = CE_x.shape[0]\n",
    "    \n",
    "    # Inicialice los parametros con ceros \n",
    "    w, b = inicializa_ceros(dim)\n",
    "\n",
    "    # Descenso en la dirección del gradiente (GD) \n",
    "    params, grads, costes = optimiza(w, b, CE_x, CE_y, num_iter= num_iter, tasa = tasa, print_cost = print_cost)\n",
    "    \n",
    "    # Recupere los parámetros w y b del diccionario \"params\" ##\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Prediga los ejemplos de prueba y entrenamiento (≈ 2 líneas de código)\n",
    "    YP_pred = pred(w, b, CP_x)\n",
    "    YE_pred = pred(w, b, CE_x)\n",
    "\n",
    "    # Imprima los errores de entrenamiento y prueba\n",
    "    print(\"Accuracy de entrenamiento: {} %\".format(100 - np.mean(np.abs(YE_pred - CE_y)) * 100))\n",
    "    print(\"Accuracy de prueba: {} %\".format(100 - np.mean(np.abs(YP_pred - CP_y)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"Costes\": costes,\n",
    "         \"Prediccion_prueba\": YP_pred, \n",
    "         \"Prediccion_entrenamiento\" : YE_pred, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"Tasa de aprendizaje\" : tasa,\n",
    "         \"Numero de iteraciones\": num_iter}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.7\n",
    "\n",
    "De qué dimensiones deben ser las matrices con los datos de entrada y de salida?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 61), (600,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE_x.shape, CE_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de entrada tiene 600 observaciones con 61 predictores y la salida es un vector columna de tamaño 600, ya que si tenemos $n$ observaciones tendremos $n$ predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 600) (1, 600)\n"
     ]
    }
   ],
   "source": [
    "# Podemos re-configurar las matrices de la siguiente forma:\n",
    "CE_x2 = CE_x.T\n",
    "CP_x2 = CP_x.T\n",
    "CE_y2 = np.array(CE_y)[np.newaxis]\n",
    "CP_y2 = np.array(CP_y)[np.newaxis]\n",
    "\n",
    "print(CE_x2.shape, CE_y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ejecute la siguiente celda para entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: nan\n",
      "Coste tras la iteración 100: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 200: nan\n",
      "Coste tras la iteración 300: nan\n",
      "Coste tras la iteración 400: nan\n",
      "Coste tras la iteración 500: nan\n",
      "Coste tras la iteración 600: nan\n",
      "Coste tras la iteración 700: nan\n",
      "Coste tras la iteración 800: nan\n",
      "Coste tras la iteración 900: nan\n",
      "Accuracy de entrenamiento: 42.833333333333336 %\n",
      "Accuracy de prueba: 40.5 %\n"
     ]
    }
   ],
   "source": [
    "d = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 1000, tasa = 1e-6, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\"> \n",
    "<tr>\n",
    "<td> Coste tras la iteración 0   </td> \n",
    "<td> 0.693147 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "</tr>  \n",
    "<tr>\n",
    "<td> Precisión de entrenamiento  </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Precisión de prueba </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión de entrenamiento es muy similar a la que conseguimos mediante la regresion logistica. También podemos observar que el error de prueba es igual al de entrenamiento. Este resultado sugiere que el modelo aprende segun entrenamiento, y generaliza de igual forma sobre los observaciones nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfica de la curva de aprendizaje (con costes)\n",
    "costes = np.squeeze(d['Costes'])\n",
    "plt.plot(costes)\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "plt.title(\"Tasa de aprendizaje =\" + str(d[\"Tasa de aprendizaje\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación**:\n",
    "Se puede ver el coste decreciendo, demostrando que los parámetros están siendo aprendidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos un primer modelo de clasificación. Ahora examinemos distintos valores para la tasa de aprendizaje $\\alpha$. \n",
    "\n",
    "#### Selección de la tasa de aprendizaje ####\n",
    "\n",
    "Para que el método del GD funcione de manera adecuada, se debe elegir la tasa de aprendiazaje de manera acertada. Esta tasa $\\alpha$  determina qué tan rápido se actualizan los parámetros. Si la tasa es muy grande se puede \"sobrepasar\" el valor óptimo. Y de manera similar, si es muy pequeña se van a necesitar muchas iteraciones para converger a los mejores valores. Por ello la importancia de tener una tase de aprensizaje bien afinada.  \n",
    "\n",
    "Ahora, comparemos la curva de aprendizaje de nuestro modelo con distintas elecciones para $\\alpha$. Ejecute el código abajo. También puede intentar con valores distintos a los tres que estamos utilizando abajo para `tasas` y analize los resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tasa de aprendizaje es: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de entrenamiento: 68.83333333333334 %\n",
      "Accuracy de prueba: 68.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-06\n",
      "Accuracy de entrenamiento: 42.833333333333336 %\n",
      "Accuracy de prueba: 40.5 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-10\n",
      "Accuracy de entrenamiento: 30.0 %\n",
      "Accuracy de prueba: 30.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 2e-20\n",
      "Accuracy de entrenamiento: 30.0 %\n",
      "Accuracy de prueba: 30.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasas = [1e-4, 1e-6, 1e-10, 2e-20]\n",
    "modelos = {}\n",
    "for i in tasas:\n",
    "    print (\"La tasa de aprendizaje es: \" + str(i))\n",
    "    modelos[str(i)] = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 2000, tasa = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in tasas:\n",
    "    plt.plot(np.squeeze(modelos[str(i)][\"Costes\"]), label= str(modelos[str(i)][\"Tasa de aprendizaje\"]))\n",
    "\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.8\n",
    "\n",
    "Analice los resultados, con cuál tasa de aprendizaje intentaría mejorar el desempeño del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def inicializa_ceros(dim):\n",
    "    \"\"\"\n",
    "    Esta función crea un vector de ceros de dimensión (dim, 1) para w e inicializa b a 0.\n",
    "    Input:\n",
    "    dim: tamaño del vector w (número de parámetros para este caso)\n",
    "    Output:\n",
    "    w: vector inicializado de tamaño (dim, 1)\n",
    "    b: escalar inicializado (corresponde con el sesgo)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.random.rand(dim,1)\n",
    "    b = 1\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasa de aprendizaje:[0.08455458]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.04265683]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.08998333]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.03817318]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.01265765]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.07423112]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.01810296]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.0977874]\n",
      "Accuracy de entrenamiento: 33.16666666666667 %\n",
      "Accuracy de prueba: 33.5 %\n",
      "tasa de aprendizaje:[0.01290736]\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "tasa de aprendizaje:[0.03559175]\n",
      "Accuracy de entrenamiento: 33.16666666666667 %\n",
      "Accuracy de prueba: 33.5 %\n"
     ]
    }
   ],
   "source": [
    "for i in np.random.rand(10,1)/10:\n",
    "    print(\"tasa de aprendizaje:\" + str(i))\n",
    "    modelos[str(i)] = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 20000, tasa = i, print_cost = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ajustamos el modelo logístico y lo probamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logT = LogisticRegression(penalty='none', max_iter=1500)\n",
    "logT.fit(CE_x, CE_y)\n",
    "y_tr = logT.predict(CE_x)\n",
    "y_pred = logT.predict(CP_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos los coeficientes del modelo de la neurona sigmoide y su desviación con respecto a la estimación tradicional de regresion logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logT_coef' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1b3f462b980a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mastropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mTabla\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogT_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regresion logistica\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Neurona sigmoide\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Diferencia\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mTabla\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logT_coef' is not defined"
     ]
    }
   ],
   "source": [
    "from astropy.table import QTable, Table, Column\n",
    "\n",
    "Tabla =  Table([logT_coef.T, d['w'], x.T], names=(\"Regresion logistica\", \"Neurona sigmoide\", \"Diferencia\"))\n",
    "Tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3.1\n",
    "\n",
    "Qué puede observar en esta comparativa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la exactitud de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La neurona sigmoide tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((d['Prediccion_entrenamiento'] == CE_y2).mean())) +\" y de validacion: \" +str(float((d['Prediccion_prueba'] == CP_y2).mean())))\n",
    "print(\"La regresion tradicional tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((y_tr == CE_y).mean())) +\" y de validacion: \" +str(float((y_pred == CP_y).mean())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio  3.2\n",
    "\n",
    "Ahora puede desarrollar su propio código intentando mejorar los resultados obtenidos. \n",
    "\n",
    "Intente sobrepasar los resultados de la regresion logistica tradicional. Optimice la tasa de aprendizaje, el número de iteraciones o (bono) investigue y cambie la manera en la cual inicializamos los coeficientes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
